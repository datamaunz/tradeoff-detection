{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/datamaunz/tradeoff-detection/blob/main/Tradeoff_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installations"
      ],
      "metadata": {
        "id": "aBVHIEDnLVLL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BfpZBEqGYi5L"
      },
      "outputs": [],
      "source": [
        "!pip install sentence-transformers\n",
        "!pip install kaleido\n",
        "!pip install plotly\n",
        "!python -m spacy download en_core_web_sm\n",
        "!pip install xlsxwriter"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make sure to use a GPU"
      ],
      "metadata": {
        "id": "lPXwGYSwLYyD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d3mtGr_DXhr8"
      },
      "outputs": [],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "7RZEgVe_Ldkg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pWAtII6ub9vQ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import spacy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "import time\n",
        "from scipy.cluster.hierarchy import linkage, dendrogram, fcluster\n",
        "from scipy.spatial.distance import squareform\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.graph_objects as go\n",
        "from xlsxwriter import Workbook\n",
        "\n",
        "from plotly.subplots import make_subplots"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load models"
      ],
      "metadata": {
        "id": "BMWvKLALLhAA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NBU8Qu_Jyf3z"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "modelName = \"paraphrase-distilroberta-base-v1\"\n",
        "model = SentenceTransformer(modelName)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1K_BtOm0RA7"
      },
      "source": [
        "## Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Label identification"
      ],
      "metadata": {
        "id": "ihVKxwOTLorN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "0sCfZtHoLkqn"
      },
      "outputs": [],
      "source": [
        "#### Functions for label identification\n",
        "\n",
        "def add_stopwords(stopwords_to_be_added):\n",
        "  \n",
        "  for stopword in stopwords_to_be_added:\n",
        "    STOP_WORDS.add(stopword)\n",
        "  for word in STOP_WORDS:\n",
        "      lexeme = nlp.vocab[word]\n",
        "      lexeme.is_stop = True\n",
        "\n",
        "def sample_comments_and_concatenate_as_string(df, column, number_samples, sep, random_seed):\n",
        "  sample = df[column].sample(number_samples, random_state=random_seed).dropna().to_list()\n",
        "  sample_string = f\"{sep}\".join(sample)\n",
        "  return sample_string\n",
        "\n",
        "def get_concatenated_preprocessed_sampled_strings(df, number_of_sampled_comments, random_seed):\n",
        "\n",
        "  start = time.time()\n",
        "  cons_items_string = sample_comments_and_concatenate_as_string(df, \"cons\", number_of_sampled_comments, \"\\n\\n\", random_seed)\n",
        "  pros_items_string = sample_comments_and_concatenate_as_string(df, \"pros\", number_of_sampled_comments, \"\\n\\n\", random_seed)\n",
        "\n",
        "  cons_string_doc = nlp(cons_items_string)\n",
        "  pros_string_doc = nlp(pros_items_string)\n",
        "\n",
        "  end = time.time()\n",
        "  print(end - start)\n",
        "\n",
        "  return cons_string_doc, pros_string_doc\n",
        "\n",
        "def get_list_of_noun_chunks(cons_string_doc, pros_string_doc):\n",
        "\n",
        "  cons_string_chunks = [x.lemma_ for x in cons_string_doc.noun_chunks]\n",
        "  pros_string_chunks = [x.lemma_ for x in pros_string_doc.noun_chunks]\n",
        "\n",
        "  chunks = cons_string_chunks + pros_string_chunks\n",
        "  return chunks\n",
        "\n",
        "def chunks_to_docs(chunks):\n",
        "\n",
        "  start = time.time()\n",
        "  chunks_docs = [nlp(chunk) for chunk in chunks]\n",
        "  end = time.time()\n",
        "  print(end - start)\n",
        "  return chunks_docs\n",
        "\n",
        "def present_nouns_in_df(chunks_docs):\n",
        "\n",
        "  cleaned_chunks = [\" \".join([token.lemma_.lower() for token in doc if (token.pos_ not in [\"ADJ\"]) & (token.is_stop == False) & (token.is_punct == False)]) for doc in chunks_docs]\n",
        "  cleaned_chunks = [x for x in cleaned_chunks if x != \"\"]\n",
        "  df = pd.DataFrame(Counter(cleaned_chunks), index=[0]).T.sort_values(0, ascending=False).reset_index().rename(columns={\"index\":\"lemma\", 0:\"COUNT\"})\n",
        "  return df\n",
        "\n",
        "def get_most_frequent_nouns_and_their_similarities(noun_df, number_of_most_frequent_words):\n",
        "\n",
        "  frequent_words = noun_df.lemma.iloc[:number_of_most_frequent_words].to_list()\n",
        "  embeddings_frequent_words = model.encode(frequent_words,convert_to_tensor=True, device=0)\n",
        "  cos_sim = util.pytorch_cos_sim(embeddings_frequent_words, embeddings_frequent_words)  \n",
        "  return cos_sim, frequent_words\n",
        "\n",
        "def plot_dendrogram_of_label_similarities(cos_sim, frequent_words):\n",
        "\n",
        "  number_of_labels = len(frequent_words)\n",
        "  plt.rcParams['lines.linewidth'] = 5\n",
        "\n",
        "  correlations = pd.DataFrame(cos_sim).applymap(lambda x: x.cpu().numpy())\n",
        "  correlations.columns = frequent_words\n",
        "  dissimilarity = 1 - abs(correlations)\n",
        "  np.fill_diagonal(dissimilarity.values, 0)\n",
        "  Z = linkage(squareform(dissimilarity), 'complete')\n",
        "\n",
        "  labels = correlations.columns\n",
        "\n",
        "  fig = plt.figure(figsize=(40,number_of_labels * 2))\n",
        "\n",
        "  ax = fig.add_subplot(1, 1, 1)\n",
        "  fig.subplots_adjust(bottom = 0.5)\n",
        "  dendrogram(Z, ax=ax, \n",
        "            orientation=\"left\", \n",
        "            labels=labels, color_threshold=0.25\n",
        "            )\n",
        "  ax.tick_params(axis='x', which='major', labelsize=35)\n",
        "  ax.tick_params(axis='y', which='major', labelsize=35, pad=20, right=100)\n",
        "\n",
        "def map_labels_by_similarity_threshold(cos_sim, noun_df, frequent_words, dissimilarity_threshold):\n",
        "\n",
        "  correlations = pd.DataFrame(cos_sim).applymap(lambda x: x.cpu().numpy())\n",
        "\n",
        "  dissimilarity = 1 - abs(correlations)\n",
        "  np.fill_diagonal(dissimilarity.values, 0)\n",
        "  Z = linkage(squareform(dissimilarity), 'complete')\n",
        "  labels_clust = fcluster(Z, dissimilarity_threshold, criterion='distance')\n",
        "  mapping = np.column_stack([frequent_words, labels_clust])\n",
        "  columns_df = ['lemma', 'cluster_label']\n",
        "  dfmapping = pd.DataFrame(mapping, columns=columns_df)\n",
        "  dfmapping = pd.merge(dfmapping, noun_df, left_on=\"lemma\", right_on=\"lemma\")\n",
        "\n",
        "  return dfmapping\n",
        "\n",
        "def reduce_similar_labels_top_most_frequent(dfmapping):\n",
        "\n",
        "  lemmata = []\n",
        "  cluster_labels = []\n",
        "  for cluster_label in dfmapping.cluster_label.unique():\n",
        "    sub_df = dfmapping[dfmapping.cluster_label == cluster_label]\n",
        "    \n",
        "    lemma_1 = sub_df.sort_values(\"COUNT\", ascending=False).iloc[0].lemma\n",
        "    lemma_2 = sub_df.sort_values(\"COUNT\", ascending=False).iloc[-1].lemma\n",
        "\n",
        "    # use the label with more examples unless it is contained in another label\n",
        "\n",
        "    if len(dfmapping[dfmapping.lemma.str.contains(lemma_1)]) > len(dfmapping[dfmapping.lemma.str.contains(lemma_2)]):\n",
        "      lemmata.append(lemma_2)\n",
        "    else:\n",
        "      lemmata.append(lemma_1)\n",
        "\n",
        "    cluster_labels.append(cluster_label)\n",
        "\n",
        "  label_df = pd.DataFrame(\n",
        "      {\n",
        "          \"lemma\":lemmata,\n",
        "      \"cluster_label\":cluster_labels,\n",
        "      }\n",
        "  )\n",
        "\n",
        "  return label_df\n",
        "\n",
        "def get_most_representative_labels(df, number_of_sampled_comments, number_of_most_frequent_words, labels_to_be_explicitly_ignored,dissimilarity_threshold, stopwords_to_be_added, random_seed, use_similarity_reduction=True, show_similarity_dendrogram=False):\n",
        "\n",
        "  cons_string_doc, pros_string_doc = get_concatenated_preprocessed_sampled_strings(df, number_of_sampled_comments, random_seed)\n",
        "  chunks = get_list_of_noun_chunks(cons_string_doc, pros_string_doc)\n",
        "  add_stopwords(stopwords_to_be_added)\n",
        "  chunks_docs = chunks_to_docs(chunks)\n",
        "  noun_df = present_nouns_in_df(chunks_docs)\n",
        "  noun_df = noun_df[noun_df.lemma.isin(labels_to_be_explicitly_ignored) == False]\n",
        "  cos_sim, frequent_words = get_most_frequent_nouns_and_their_similarities(noun_df, number_of_most_frequent_words)\n",
        "  if show_similarity_dendrogram == True: \n",
        "    plot_dendrogram_of_label_similarities(cos_sim, frequent_words)\n",
        "  if use_similarity_reduction == True:\n",
        "    dfmapping = map_labels_by_similarity_threshold(cos_sim, noun_df, frequent_words, dissimilarity_threshold)\n",
        "    label_df = reduce_similar_labels_top_most_frequent(dfmapping)\n",
        "    labels = label_df.lemma.to_list()\n",
        "  else:\n",
        "    labels = frequent_words\n",
        "  return labels, noun_df"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Identify pro-con pairs"
      ],
      "metadata": {
        "id": "2nq3y7QbL1Aj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "WBTN4u5wjvaL"
      },
      "outputs": [],
      "source": [
        "def create_similarity_frame(embeddings_comments, embeddings_labels, labels, comments, reviewIDs, reviewType):\n",
        "  cos_sim = util.pytorch_cos_sim(embeddings_comments, embeddings_labels)  \n",
        "  df = pd.DataFrame(cos_sim, columns=labels)\n",
        "  df = df.applymap(lambda x: x.cpu().numpy())\n",
        "  df[f\"{reviewType}_label\"] = df.idxmax(axis=1)\n",
        "  df[f\"{reviewType}_score\"] = df.max(axis=1)\n",
        "  df[f'{reviewType}_comment'] = comments\n",
        "  df[\"reviewID\"] = reviewIDs\n",
        "  df = df.sort_values(f\"{reviewType}_score\", ascending=False)\n",
        "  return df\n",
        "\n",
        "def from_embeddings_to_labeled_df(embeddings_comments, embeddings_labels, labels):\n",
        "  cos_sim = util.pytorch_cos_sim(embeddings_comments, embeddings_labels)  \n",
        "  df = pd.DataFrame(cos_sim, columns=labels)\n",
        "  df = df.applymap(lambda x: x.cpu().numpy())\n",
        "  return df\n",
        "\n",
        "def all_labels_above_sim_thresh(df, sim_thresh, reviewIDs, reviewType):\n",
        "  df = df[df >= sim_thresh]\n",
        "  df[\"reviewID\"] = reviewIDs\n",
        "  df = df.dropna(thresh=2)\n",
        "  df = df.set_index(\"reviewID\").unstack().reset_index().dropna()\n",
        "  df = df.rename(columns={\n",
        "      0:f'{reviewType}_score',\n",
        "      'level_0':f'{reviewType}_label'\n",
        "      })\n",
        "  return df\n",
        "\n",
        "def create_similarity_frame_with_multiple_labels_per_comment(embeddings_comments, embeddings_labels, labels, sim_thresh, reviewIDs, reviewType):\n",
        "\n",
        "  df = from_embeddings_to_labeled_df(embeddings_comments, embeddings_labels, labels)\n",
        "  df = all_labels_above_sim_thresh(df, sim_thresh, reviewIDs, reviewType)\n",
        "  return df\n",
        "\n",
        "def bootstrap_contradiction_counts(pros_cons_df, random_seed, number_of_iterations):\n",
        "  frames = [pd.DataFrame(Counter(np.random.RandomState(seed=random_seed).permutation(pros_cons_df[\"cons_label\"]) + \" | \" + np.random.RandomState(seed=random_seed).permutation(pros_cons_df[\"pros_label\"])), index=[0]).T for i in range(number_of_iterations)]\n",
        "  bootstrapped_contra_counts_df = pd.concat(frames, axis=1).fillna(0)\n",
        "  return bootstrapped_contra_counts_df\n",
        "\n",
        "def bootstrap_contradiction_counts_with_seed(pros_cons_df, number_of_iterations, random_seed):\n",
        "  frames = []\n",
        "  for iteration in range(number_of_iterations):\n",
        "    frame = pd.DataFrame(Counter(np.random.RandomState(seed=iteration).permutation(pros_cons_df[\"cons_label\"]) + \" | \" + np.random.RandomState(seed=iteration+random_seed).permutation(pros_cons_df[\"pros_label\"])), index=[0]).T\n",
        "    frames.append(frame)\n",
        "  bootstrapped_contra_counts_df = pd.concat(frames, axis=1).fillna(0)\n",
        "  return bootstrapped_contra_counts_df\n",
        "\n",
        "\n",
        "def analyze_bootstraped_contradiction_counts(pros_cons_df, bootstrapped_contra_counts_df, zscore):\n",
        "  bootstrapped_contra_df = pd.concat([bootstrapped_contra_counts_df.std(axis=1), bootstrapped_contra_counts_df.mean(axis=1)], axis=1)\n",
        "  bootstrapped_contra_df.columns = [\"SD\", \"MEAN\"]\n",
        "  bootstrapped_contra_df[\"low\"] = bootstrapped_contra_df[\"MEAN\"] - bootstrapped_contra_df[\"SD\"] * zscore\n",
        "  bootstrapped_contra_df[\"high\"] = bootstrapped_contra_df[\"MEAN\"] + bootstrapped_contra_df[\"SD\"] * zscore\n",
        "  return bootstrapped_contra_df\n",
        "\n",
        "def create_p_value_dict_for_contradictions(pros_cons_df, bootstrapped_contra_df, number_of_iterations):\n",
        "  contra_count_df = pros_cons_df.groupby(\"contradiction\").count()[[\"reviewID\"]].rename(columns={\"reviewID\":\"COUNT\"})\n",
        "\n",
        "  p_value_high_dict = {}\n",
        "  p_value_low_dict = {}\n",
        "  for index, row in bootstrapped_contra_df.iterrows():\n",
        "    if index in contra_count_df.index:\n",
        "      occurences = contra_count_df.loc[index][\"COUNT\"]\n",
        "    else:\n",
        "      occurences = 0\n",
        "    p_value_high_dict[index] = len([x for x in row.values if x >= occurences]) / number_of_iterations\n",
        "    p_value_low_dict[index] = len([x for x in row.values if x <= occurences]) / number_of_iterations\n",
        "  \n",
        "  return p_value_high_dict, p_value_low_dict\n",
        "\n",
        "def add_p_values_for_most_frequent_combinations(pros_cons_df, bootstrapped_contra_df, p_value_dict):\n",
        "  contra_count_df = pros_cons_df.groupby(\"contradiction\").count()[[\"reviewID\"]].rename(columns={\"reviewID\":\"COUNT\"})\n",
        "  \n",
        "  frame = pd.merge(contra_count_df, bootstrapped_contra_df, left_index=True, right_index=True)\n",
        "  for index, row in frame.iterrows():\n",
        "    frame.loc[index, \"p_value\"] = p_value_dict.get(index)\n",
        "  return frame\n",
        "\n",
        "def create_df_with_contradictions(pros_df, cons_df):\n",
        "\n",
        "  pros_cons_df = pd.merge(pros_df, cons_df, left_on=\"reviewID\", right_on=\"reviewID\", how=\"outer\").dropna()\n",
        "  pros_cons_df[\"contradiction\"] = pros_cons_df[\"cons_label\"] + \" | \" + pros_cons_df[\"pros_label\"]\n",
        "  return pros_cons_df\n",
        "\n",
        "def identify_significant_contradictions(pros_cons_df, random_seed, number_of_iterations, zscore=1.96, p_value = 0.05):\n",
        "  #bootstrapped_contra_counts_df = bootstrap_contradiction_counts(pros_cons_df, random_seed, number_of_iterations)\n",
        "\n",
        "  bootstrapped_contra_counts_df = bootstrap_contradiction_counts_with_seed(pros_cons_df, number_of_iterations, random_seed)\n",
        "  bootstrapped_contra_df = analyze_bootstraped_contradiction_counts(pros_cons_df, bootstrapped_contra_counts_df, zscore)\n",
        "  p_value_high_dict, p_value_low_dict = create_p_value_dict_for_contradictions(pros_cons_df, bootstrapped_contra_counts_df, number_of_iterations)\n",
        "  pros_cons_with_p_values_high_df = add_p_values_for_most_frequent_combinations(pros_cons_df, bootstrapped_contra_df, p_value_high_dict)\n",
        "  pros_cons_with_p_values_low_df = add_p_values_for_most_frequent_combinations(pros_cons_df, bootstrapped_contra_df, p_value_low_dict)\n",
        "  #high_df = pros_cons_with_p_values_df[(pros_cons_with_p_values_df[\"COUNT\"] > pros_cons_with_p_values_df[\"high\"])].sort_index()\n",
        "  #high_df = pros_cons_with_p_values_df[(pros_cons_with_p_values_df[\"p_value\"] < p_value)].sort_index()\n",
        "  return pros_cons_with_p_values_high_df, pros_cons_with_p_values_low_df\n",
        "\n",
        "def pair_contra_pairs_into_same_row(high_df):\n",
        "\n",
        "  high_df = high_df.reset_index()\n",
        "  high_df[\"pair_values\"] = high_df[\"index\"].apply(lambda x: \"_\".join(sorted(x.split(\" | \"))))\n",
        "\n",
        "  pair_dfs = [high_df[high_df.pair_values == pair] for pair in high_df.pair_values.unique() if len(high_df[high_df.pair_values == pair]) > 1]\n",
        "\n",
        "  dfs = []\n",
        "  for pair_df in pair_dfs:\n",
        "    pair_1_df = pd.DataFrame(pair_df.iloc[0]).T.reset_index(drop=True)\n",
        "    pair_1_df.columns = [f'{x}_1' for x in pair_1_df.columns]\n",
        "\n",
        "    pair_2_df = pd.DataFrame(pair_df.iloc[1]).T.reset_index(drop=True)\n",
        "    pair_2_df.columns = [f'{x}_2' for x in pair_2_df.columns]\n",
        "\n",
        "    dfs.append(pd.concat([pair_1_df, pair_2_df], axis=1))\n",
        "  \n",
        "  if len(dfs) > 0:\n",
        "    pair_df = pd.concat(dfs)\n",
        "    pair_df = pair_df[pair_df.duplicated() == False]\n",
        "    pair_df = pair_df.reset_index(drop=True)\n",
        "  else: \n",
        "    pair_df = pd.DataFrame()\n",
        "  return pair_df\n",
        "\n",
        "def smooth_adjusted_p_values(p_value_adjust_df):\n",
        "\n",
        "  for index, row in p_value_adjust_df.reset_index().iterrows():\n",
        "    if index > 0:\n",
        "      if row.p_value_adjusted < p_value_adjust_df.loc[index - 1].p_value_adjusted:\n",
        "        p_value_adjust_df.loc[index, \"p_value_adjusted\"] = p_value_adjust_df.loc[index - 1].p_value_adjusted\n",
        "\n",
        "  return p_value_adjust_df\n",
        "\n",
        "def adjust_p_values_with_holm_bonferroni(labels, labels_to_be_filtered, pros_cons_with_p_values_df, p_value):\n",
        "\n",
        "  n_labels = len(labels) - len(labels_to_be_filtered)\n",
        "  m = (n_labels)**2 - n_labels\n",
        "  p_value_adjust_df = pros_cons_with_p_values_df.copy()\n",
        "  p_value_adjust_df = p_value_adjust_df.sort_values(\"p_value\")\n",
        "  p_value_adjust_df = p_value_adjust_df.reset_index()\n",
        "  p_value_adjust_df = p_value_adjust_df[p_value_adjust_df[\"index\"].apply(lambda x: x.split(\" | \")[0] == x.split(\" | \")[1]) == False].set_index(\"index\")\n",
        "  p_value_adjust_df[\"rank\"] = np.arange(1,len(p_value_adjust_df)+1)\n",
        "  p_value_adjust_df[\"p_value_adjusted\"] = (m + 1 - p_value_adjust_df[\"rank\"]) * p_value_adjust_df[\"p_value\"]\n",
        "  p_value_adjust_df = p_value_adjust_df.reset_index()\n",
        "  p_value_adjust_df = smooth_adjusted_p_values(p_value_adjust_df)\n",
        "  \n",
        "  comp_adjust_df = p_value_adjust_df[p_value_adjust_df[\"p_value_adjusted\"] < p_value]\n",
        "  \n",
        "  return p_value_adjust_df, comp_adjust_df\n",
        "\n",
        "\n",
        "def obtain_sector_contradictions(embeddings_pros, embeddings_cons, embeddings_labels_dict, labels, labels_to_be_filtered, reviewIDs, sim_thresh, p_value, random_seed, number_of_iterations, zscore=1.96):\n",
        "\n",
        "  pros_df = create_similarity_frame_with_multiple_labels_per_comment(embeddings_comments = embeddings_pros, embeddings_labels = embeddings_labels_dict[\"embeddings_labels_pro\"], labels = labels, sim_thresh=sim_thresh, reviewIDs = reviewIDs, reviewType = \"pros\")\n",
        "  cons_df = create_similarity_frame_with_multiple_labels_per_comment(embeddings_comments = embeddings_cons, embeddings_labels = embeddings_labels_dict[\"embeddings_labels_con\"], labels = labels, sim_thresh=sim_thresh, reviewIDs = reviewIDs, reviewType = \"cons\")\n",
        "  pros_cons_df = create_df_with_contradictions(pros_df, cons_df)\n",
        "  reviewIDs_to_exclude = pros_cons_df[(pros_cons_df.pros_label.isin(labels_to_be_filtered) == True) | (pros_cons_df.cons_label.isin(labels_to_be_filtered) == True)][\"reviewID\"]\n",
        "  pros_cons_df = pros_cons_df[pros_cons_df.reviewID.isin(reviewIDs_to_exclude) == False]\n",
        "\n",
        "  pros_cons_with_p_values_high_df, pros_cons_with_p_values_low_df = identify_significant_contradictions(pros_cons_df, random_seed, number_of_iterations, zscore=zscore, p_value = p_value)\n",
        "  \n",
        "  p_value_high_adjust_df, high_adjust_df = adjust_p_values_with_holm_bonferroni(labels, labels_to_be_filtered, pros_cons_with_p_values_high_df, p_value)\n",
        "  p_value_low_adjust_df, low_adjust_df = adjust_p_values_with_holm_bonferroni(labels, labels_to_be_filtered, pros_cons_with_p_values_low_df, p_value)\n",
        "\n",
        "  #pair_high_df = pair_contra_pairs_into_same_row(high_df)\n",
        "  pair_high_adjust_df = pair_contra_pairs_into_same_row(high_adjust_df)\n",
        "\n",
        "  #pair_low_df = pair_contra_pairs_into_same_row(high_df)\n",
        "  pair_low_adjust_df = pair_contra_pairs_into_same_row(low_adjust_df)\n",
        "\n",
        "  return pros_df, cons_df, pros_cons_df, pros_cons_with_p_values_high_df, pros_cons_with_p_values_low_df, pair_high_adjust_df, pair_low_adjust_df, p_value_high_adjust_df, p_value_low_adjust_df, high_adjust_df, low_adjust_df, pair_high_adjust_df, pair_low_adjust_df"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tradeoffs in relation to companies"
      ],
      "metadata": {
        "id": "sVUA_5XuMDr3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def complete_tradeoff_positions_count_df(tradeoff_positions_count_df, contradictions_in_companies_df, max_firms):\n",
        "\n",
        "  for contra in list(set(tradeoff_positions_count_df.index.get_level_values(0))):\n",
        "    contra_frame = contradictions_in_companies_df[contradictions_in_companies_df[\"sector_contradiction_1\"] == contra]\n",
        "    num_of_companies_with_contra = len(contra_frame)\n",
        "\n",
        "    sub_frame_1 = pd.DataFrame({\"sector_contradiction\":[contra], \"significant_conflict\":[\"both\"], \"COUNT\":[num_of_companies_with_contra]}).set_index([\"sector_contradiction\", \"significant_conflict\"])\n",
        "    \n",
        "    for item in list(set(tradeoff_positions_count_df.loc[contra].index)):\n",
        "      \n",
        "      tradeoff_positions_count_df.loc[contra, item][\"COUNT\"] = tradeoff_positions_count_df.loc[contra, item][\"COUNT\"] - num_of_companies_with_contra\n",
        "    \n",
        "    number_of_companies_with_trait = tradeoff_positions_count_df.loc[contra][\"COUNT\"].sum()\n",
        "\n",
        "    sub_frame_2 = pd.DataFrame({\"sector_contradiction\":[contra], \"significant_conflict\":[\"neither nor\"], \"COUNT\":[max_firms - number_of_companies_with_trait - 2*num_of_companies_with_contra]}).set_index([\"sector_contradiction\", \"significant_conflict\"])\n",
        "\n",
        "    tradeoff_positions_count_df = pd.concat([tradeoff_positions_count_df, sub_frame_1, sub_frame_2])\n",
        "\n",
        "  tradeoff_positions_count_df = tradeoff_positions_count_df.sort_index()\n",
        "  return tradeoff_positions_count_df\n",
        "\n",
        "\n",
        "def create_df_with_contradictions_within_companies(companies_high_df, company_names):\n",
        "\n",
        "  company_pair_dfs = []\n",
        "\n",
        "  for company_name in company_names:\n",
        "    company_high_df = companies_high_df[companies_high_df.company_name == company_name]\n",
        "    if len(company_high_df) > 0:\n",
        "      company_pair_df = pair_contra_pairs_into_same_row(company_high_df)\n",
        "      company_pair_dfs.append(company_pair_df)\n",
        "\n",
        "  if len(company_pair_dfs) > 0:\n",
        "    contradictions_in_companies_df = pd.concat(company_pair_dfs)\n",
        "  else: contradictions_in_companies_df = pd.DataFrame()\n",
        "\n",
        "  return contradictions_in_companies_df\n",
        "\n",
        "def identify_conflicts_in_n_largest_firms(df, sector_contradictions, company_names, labels, labels_to_be_filtered, labels_to_be_explicitly_ignored, sim_thresh):\n",
        "\n",
        "  company_tradeoff_pos_dfs = []\n",
        "\n",
        "  for company_name in company_names:\n",
        "    print(company_name)\n",
        "\n",
        "    company_df = df[df[\"employerName\"] == company_name]\n",
        "\n",
        "    company_reviewIDs = company_df[\"reviewID\"].to_list()\n",
        "    company_pros = company_df.pros.to_list()\n",
        "    company_cons = company_df.cons.to_list()\n",
        "\n",
        "    company_embeddings_pros = model.encode(company_pros, convert_to_tensor=True, device=0)\n",
        "    company_embeddings_cons = model.encode(company_cons, convert_to_tensor=True, device=0)\n",
        "    #embeddings_labels = model.encode(labels,convert_to_tensor=True, device=0)\n",
        "    #sim_thresh = 0.35\n",
        "\n",
        "    company_pros_df, company_cons_df, company_pros_cons_df, company_high_df, company_pair_df = obtain_sector_contradictions(company_embeddings_pros, company_embeddings_cons, embeddings_labels, labels, labels_to_be_filtered, labels_to_be_explicitly_ignored, company_reviewIDs, sim_thresh)\n",
        "    company_pair_df[\"company_name\"] = company_name\n",
        "    company_pair_df[\"sim_thresh\"] = sim_thresh\n",
        "\n",
        "    company_high_df = company_high_df.reset_index().rename(columns={\"index\":\"significant_conflict\"})\n",
        "\n",
        "    pair_sides = []\n",
        "    for sector_contradiction in sector_contradictions:\n",
        "\n",
        "      pair = sector_contradiction.split(\"_\")\n",
        "      pair_side = company_high_df[(company_high_df[\"significant_conflict\"].str.contains(pair[0])) & (company_high_df[\"significant_conflict\"].str.contains(pair[-1]))]\n",
        "      #pair_side = company_high_df[(company_high_df[\"significant_conflict\"].isin([pair[0]])) & (company_high_df[\"significant_conflict\"].isin([pair[-1]]))]\n",
        "      pair_side[\"sector_contradiction\"] = sector_contradiction\n",
        "      pair_sides.append(pair_side)\n",
        "\n",
        "    company_tradeoff_pos_df = pd.concat(pair_sides)\n",
        "    company_tradeoff_pos_df[\"company_name\"] = company_name\n",
        "\n",
        "    company_tradeoff_pos_dfs.append(company_tradeoff_pos_df)\n",
        "\n",
        "  tradeoff_positions_df = pd.concat(company_tradeoff_pos_dfs)\n",
        "  return tradeoff_positions_df\n",
        "\n",
        "def retrieve_n_largest_firms_df(df, max_firms):\n",
        "\n",
        "  firm_review_count_df = df.groupby(\"employerName\").count()[[\"reviewID\"]].rename(columns={\"reviewID\":\"review_count\"})\n",
        "  n_largest_firms_df = firm_review_count_df.sort_values(\"review_count\", ascending=False).iloc[:max_firms]\n",
        "  return n_largest_firms_df\n",
        "\n",
        "def add_missing_pair(tradeoff_positions_count_df):\n",
        "  sub_frames = []\n",
        "\n",
        "  sector_contradictions = tradeoff_positions_count_df.index.get_level_values(0).unique()\n",
        "  for sector_contradiction in sector_contradictions:\n",
        "\n",
        "    sub_frame = tradeoff_positions_count_df.loc[sector_contradiction]\n",
        "    if len(sub_frame) == 3:\n",
        "      sub_frame = sub_frame.reset_index()\n",
        "      pair = [x for x in sub_frame.significant_conflict.to_list() if \"|\" in x][0]\n",
        "      missing_pair = f'{pair.split(\" | \")[-1]} | {pair.split(\" | \")[0]}'\n",
        "      \n",
        "      sub_frame = sub_frame.append({\"significant_conflict\":missing_pair, \"COUNT\":0}, ignore_index=True)\n",
        "      sub_frame = sub_frame.reset_index(drop=True)\n",
        "      sub_frame[\"sector_contradiction\"] = sector_contradiction\n",
        "      sub_frames.append(sub_frame)\n",
        "    else:\n",
        "      sub_frame = sub_frame.reset_index()\n",
        "      sub_frame[\"sector_contradiction\"] = sector_contradiction\n",
        "      sub_frames.append(sub_frame)\n",
        "\n",
        "\n",
        "  return pd.concat(sub_frames).set_index([\"sector_contradiction\", \"significant_conflict\"])\n",
        "  \n"
      ],
      "metadata": {
        "id": "E7ZeKR9SSnAQ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_examples_for_conflicts_in_contradictions(pros_cons_df, df, sector_contradictions):\n",
        "  templates_1 = [f'{x.split(\"_\")[0]} | {x.split(\"_\")[-1]}' for x in sector_contradictions]\n",
        "  templates_2 = [f'{x.split(\"_\")[-1]} | {x.split(\"_\")[0]}' for x in sector_contradictions]\n",
        "  conflicts_df = pros_cons_df[pros_cons_df.contradiction.isin(templates_1 + templates_2)]\n",
        "  conflicts_df = add_examples_to_conflicts_df(conflicts_df, df)\n",
        "  return conflicts_df\n",
        "\n",
        "def retrieve_examples_for_conflicts(pros_cons_df, df, conflicts):\n",
        "  conflicts_df = pros_cons_df[pros_cons_df.contradiction.isin(conflicts)]\n",
        "  conflicts_df = add_examples_to_conflicts_df(conflicts_df, df)\n",
        "  return conflicts_df\n",
        "\n",
        "def add_examples_to_conflicts_df(conflicts_df, df):\n",
        "\n",
        "  conflicts_df = pd.merge(conflicts_df, df[[\"reviewID\", \"cons\", \"pros\", \"employerName\"]], left_on=\"reviewID\", right_on=\"reviewID\")\n",
        "  conflicts_df = conflicts_df.rename(columns={\"contradiction\":\"contradiction_side\"})\n",
        "  conflicts_df[\"contradiction\"] = conflicts_df.contradiction_side.apply(lambda x: \"_\".join(sorted(x.split(\" | \"))))\n",
        "  conflicts_df = conflicts_df[[\"contradiction\", \"contradiction_side\", \"cons_label\", \"pros_label\", \"cons\", \"pros\", \"cons_score\", \"pros_score\", \"employerName\", \"reviewID\"]]\n",
        "  conflicts_df = conflicts_df[conflicts_df.duplicated() == False]\n",
        "  conflicts_df[\"combined_score\"] = (conflicts_df[\"cons_score\"] + conflicts_df[\"pros_score\"]) / 2 - abs(conflicts_df[\"cons_score\"] - conflicts_df[\"pros_score\"])\n",
        "\n",
        "  return conflicts_df.sort_values([\"cons_label\", \"pros_label\"], ascending=False)\n",
        "  #return conflicts_df.sort_values([\"cons_score\", \"pros_label\"], ascending=False)\n",
        "\n",
        "def add_missing_contradiction_sides(companies_contradiction_df):\n",
        "\n",
        "  company_contra_dfs = []\n",
        "\n",
        "  company_names = companies_contradiction_df.employerName.unique()\n",
        "  for company_name in company_names:\n",
        "    company_df = companies_contradiction_df[companies_contradiction_df.employerName == company_name]\n",
        "    company_contradictions = company_df.contradiction.unique()\n",
        "    for company_contradiction in company_contradictions:\n",
        "      company_contra_df = company_df[company_df.contradiction == company_contradiction]\n",
        "      \n",
        "      if len(company_contra_df) == 1:\n",
        "        contradiction_side = company_contra_df.contradiction_side.iloc[0]\n",
        "        contra_count = company_contra_df.contra_count.iloc[0]\n",
        "        \n",
        "        missing_contradiction_side = f'{contradiction_side.split(\" | \")[-1]} | {contradiction_side.split(\" | \")[0]}'\n",
        "        company_contra_df = company_contra_df.append({\n",
        "            \"employerName\":company_name, \n",
        "            \"contradiction\":company_contradiction, \n",
        "            \"contradiction_side\":missing_contradiction_side, \n",
        "            \"contra_side_count\":0,\n",
        "            \"contra_count\":contra_count\n",
        "            }, ignore_index=True)\n",
        "        company_contra_df = company_contra_df.reset_index(drop=True)\n",
        "        \n",
        "      company_contra_dfs.append(company_contra_df)\n",
        "\n",
        "  return pd.concat(company_contra_dfs)\n",
        "\n",
        "\n",
        "def retain_only_one_side_of_contradictions(companies_contradiction_df):\n",
        "\n",
        "  selected_sides = []\n",
        "\n",
        "  company_contradictions = companies_contradiction_df.contradiction.unique()\n",
        "  for company_contradiction in company_contradictions:\n",
        "    company_contradiction_sides_df = companies_contradiction_df[companies_contradiction_df.contradiction == company_contradiction]\n",
        "    company_contradiction_sides = list(company_contradiction_sides_df.contradiction_side.unique())\n",
        "    selected_sides.append(sorted(company_contradiction_sides)[0])\n",
        "  return selected_sides\n",
        "\n",
        "def filter_by_minimal_occurence(companies_contradiction_df, min_n_item):\n",
        "  #companies_contradiction_df = companies_contradiction_df[(companies_contradiction_df.contra_side_count + companies_contradiction_df.contra_count) >= min_n_item]\n",
        "  companies_contradiction_df = companies_contradiction_df[companies_contradiction_df.contra_count >= min_n_item]\n",
        "  return companies_contradiction_df\n",
        "\n",
        "def ratio_into_bin(number, bin_dict):\n",
        "  for key in bin_dict.keys():\n",
        "    if number <= key:\n",
        "      return bin_dict.get(key)\n",
        "      break\n"
      ],
      "metadata": {
        "id": "ItDJIIN6pLbG"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### pro-pro and con-con pairs"
      ],
      "metadata": {
        "id": "wCsAMKNmphmg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_review_ids_with_mulptiple_labels_in_column(df, column_type):\n",
        "  \"\"\"\n",
        "  df: [pros_df, cons_df]\n",
        "  column_type: [\"pros\", \"cons\"]\n",
        "  \"\"\"\n",
        "\n",
        "  #pros_pairs_df = pros_df.groupby([\"reviewID\", f\"{column_type}_label\"]).count().rename(columns={f\"{column_type}_score\":\"COUNT\"}).sort_values(\"COUNT\", ascending=False)\n",
        "  #pros_pairs_df = pros_pairs_df[pros_pairs_df[\"COUNT\"] > 1]\n",
        "\n",
        "  review_ids_with_multiple_labels_df = df.groupby([\"reviewID\"]).count().rename(columns={f\"{column_type}_score\":\"COUNT\"})[[\"COUNT\"]]\n",
        "  review_ids_with_multiple_labels = review_ids_with_multiple_labels_df[review_ids_with_multiple_labels_df[\"COUNT\"] > 1].index.to_list()\n",
        "\n",
        "  return review_ids_with_multiple_labels\n",
        "\n",
        "\n",
        "# identify_pairs_in_either_pros_or_cons()\n",
        "\n",
        "#review_ids_with_multiple_labels\n",
        "\n",
        "def identify_pairs_in_either_pros_or_cons_column(df, column_type, labels_to_be_filtered):\n",
        "  \"\"\"\n",
        "  df: [pros_df, cons_df]\n",
        "  column_type: [\"pros\", \"cons\"]\n",
        "  \"\"\"\n",
        "\n",
        "  review_ids_with_multiple_labels = get_review_ids_with_mulptiple_labels_in_column(df, column_type)\n",
        "  frame = df[(df.reviewID.isin(review_ids_with_multiple_labels)) & (df[f\"{column_type}_label\"].isin(labels_to_be_filtered) == False)].sort_values(\"reviewID\")\n",
        "  reviewIDs = frame.reviewID.unique()\n",
        "\n",
        "  pairs_list = []\n",
        "  for reviewID in reviewIDs:\n",
        "    label_list = frame[frame.reviewID == reviewID][f\"{column_type}_label\"].to_list()\n",
        "    pairs = list(set([l for sublist in [[\"_\".join(sorted([label, x])) for x in label_list if x != label] for label in label_list] for l in sublist]))\n",
        "    pairs_list.append(pairs)\n",
        "\n",
        "  pair_count_df = pd.DataFrame(Counter([l for sublist in pairs_list for l in sublist]), index=[\"COUNT\"]).T.sort_values(\"COUNT\", ascending=False)[[\"COUNT\"]].reset_index().rename(columns={\"index\":\"pair\"})\n",
        "  pair_count_dict = pair_count_df.set_index(\"pair\")[\"COUNT\"].to_dict()\n",
        "  return pair_count_df, frame, pair_count_dict\n",
        "\n",
        "def add_missing_bins(frame):\n",
        "  for ratio_bin in [x for x in range(1,7) if x not in frame.ratio_bin.to_list()]:\n",
        "    frame = frame.append({\"ratio_bin\":ratio_bin, \"bin_count\":0}, ignore_index=True)\n",
        "  frame[\"ratio_bin\"] = frame.ratio_bin.astype(int)\n",
        "  frame = frame.sort_values(\"ratio_bin\")\n",
        "  return frame\n",
        "\n",
        "def bootstrap_random_counts_per_pair(df, column_type, number_of_iterations):\n",
        "  \"\"\"\n",
        "  df: [pros_frame, cons_frame]\n",
        "  column_type: [\"pros\", \"cons\"]\n",
        "  \"\"\"\n",
        "  frames = []\n",
        "  for iteration in range(number_of_iterations):\n",
        "    frames.append(pd.DataFrame(Counter(df[f\"{column_type}_label\"] + \"_\" + np.random.RandomState(seed=iteration).permutation(df[f\"{column_type}_label\"])), index=[0]).T)\n",
        "\n",
        "  bootstrapped_counts_df = pd.concat(frames, axis=1)\n",
        "  bootstrapped_counts_df = bootstrapped_counts_df.fillna(0)\n",
        "  bootstrapped_counts_df = bootstrapped_counts_df.reset_index()\n",
        "\n",
        "  bootstrapped_counts_df[\"pair\"] = bootstrapped_counts_df[\"index\"].apply(lambda x: \"_\".join(sorted(x.split(\"_\"))))\n",
        "  bootstrapped_counts_df = bootstrapped_counts_df.groupby(\"pair\").sum()\n",
        "  return bootstrapped_counts_df\n",
        "\n",
        "\n",
        "\n",
        "def summarize_random_counts_metrics_for_pairs(bootstrapped_counts_df, pair_counts_dict):\n",
        "  for pair, row in bootstrapped_counts_df.iterrows():\n",
        "    if pair in pair_counts_dict.keys():\n",
        "      orig_count = pair_counts_dict.get(pair)\n",
        "\n",
        "      bootstrapped_counts_df.loc[pair, \"mean_bootstrapped\"] = row.mean()\n",
        "      bootstrapped_counts_df.loc[pair, \"std_bootstrapped\"] = row.std()\n",
        "      bootstrapped_counts_df.loc[pair, \"upper_limit_bootstrapped\"] = row.mean() + (z_score * row.std())\n",
        "      bootstrapped_counts_df.loc[pair, \"lower_limit_bootstrapped\"] = row.mean() - (z_score * row.std())\n",
        "      bootstrapped_counts_df.loc[pair, \"p_value_high\"] = len(row[row >= orig_count]) / number_of_iterations\n",
        "      bootstrapped_counts_df.loc[pair, \"p_value_low\"] = len(row[row <= orig_count]) / number_of_iterations\n",
        "\n",
        "  return bootstrapped_counts_df\n",
        "\n",
        "def identify_frequent_and_infrequent_pairs_in_column(bootstrapped_counts_df, pairs_df):\n",
        "  \"\"\"\n",
        "  pairs_df: [pros_pairs_df, cons_pairs_df]\n",
        "  \"\"\"\n",
        "\n",
        "  bootstrapped_counts_df_high = bootstrapped_counts_df[bootstrapped_counts_df.p_value_high == 0][[\"mean_bootstrapped\", \"std_bootstrapped\", \"upper_limit_bootstrapped\", \"lower_limit_bootstrapped\", \"p_value_high\", \"p_value_low\"]]\n",
        "  bootstrapped_counts_df_low = bootstrapped_counts_df[bootstrapped_counts_df.p_value_low == 0][[\"mean_bootstrapped\", \"std_bootstrapped\", \"upper_limit_bootstrapped\", \"lower_limit_bootstrapped\", \"p_value_high\", \"p_value_low\"]]\n",
        "  frequent_pairs_df = pd.merge(pairs_df, bootstrapped_counts_df_high, left_on=\"pair\", right_index=True)\n",
        "  infrequent_pairs_df = pd.merge(pairs_df, bootstrapped_counts_df_low, left_on=\"pair\", right_index=True)\n",
        "  \n",
        "\n",
        "  return frequent_pairs_df, infrequent_pairs_df\n",
        "\n",
        "def identify_frequent_and_infrequent_pairs_by_column(df, pairs_df, column_type, pair_counts_dict, number_of_iterations):\n",
        "\n",
        "  \"\"\"\n",
        "  df: [pros_frame, cons_frame]\n",
        "  column_type: [\"pros\", \"cons\"]\n",
        "  \"\"\"\n",
        "\n",
        "  bootstrapped_counts_df = bootstrap_random_counts_per_pair(df, column_type, number_of_iterations)\n",
        "  bootstrapped_counts_df = summarize_random_counts_metrics_for_pairs(bootstrapped_counts_df, pair_counts_dict)\n",
        "  frequent_pairs_df, infrequent_pairs_df = identify_frequent_and_infrequent_pairs_in_column(bootstrapped_counts_df, pairs_df)\n",
        "  frequent_pairs_df[f'{column_type}_pair'] = \"frequent\"\n",
        "  infrequent_pairs_df[f'{column_type}_pair'] = \"infrequent\"\n",
        "\n",
        "  return frequent_pairs_df, infrequent_pairs_df\n",
        "\n",
        "def create_summary_of_summary_of_significant_pairings_in_same_column(pros_frame, pros_pairs_df, cons_frame, cons_pairs_df, pros_pair_count_dict, cons_pair_count_dict, number_of_iterations):\n",
        "\n",
        "  pros_frequent_pairs_df, pros_infrequent_pairs_df = identify_frequent_and_infrequent_pairs_by_column(pros_frame, pros_pairs_df, \"pros\", pros_pair_count_dict, number_of_iterations)\n",
        "  cons_frequent_pairs_df, cons_infrequent_pairs_df = identify_frequent_and_infrequent_pairs_by_column(cons_frame, cons_pairs_df, \"cons\", cons_pair_count_dict, number_of_iterations)\n",
        "  summary_of_significant_pairings_in_same_column_df = pd.concat([cons_frequent_pairs_df, cons_infrequent_pairs_df, pros_frequent_pairs_df, pros_infrequent_pairs_df])\n",
        "\n",
        "  #summary_of_significant_pairings_in_same_column_df[\"cons_pair\"] = summary_of_significant_pairings_in_same_column_df[\"cons_pair\"].fillna(\"insignificant\")\n",
        "  #summary_of_significant_pairings_in_same_column_df[\"pros_pair\"] = summary_of_significant_pairings_in_same_column_df[\"pros_pair\"].fillna(\"insignificant\")\n",
        "  \n",
        "  return summary_of_significant_pairings_in_same_column_df\n",
        "\n",
        "def create_dict_for_pairing_frequency_classes(summary_of_significant_pairings_in_same_column_df):\n",
        "\n",
        "  pairing_dict = {}\n",
        "\n",
        "  for pair in summary_of_significant_pairings_in_same_column_df.pair.unique():\n",
        "    pairing_dict[pair] = {}\n",
        "    sub_frame = summary_of_significant_pairings_in_same_column_df[summary_of_significant_pairings_in_same_column_df.pair == pair]\n",
        "    \n",
        "    result = sub_frame[\"cons_pair\"].dropna().to_list()\n",
        "    if len(result) == 0: result = \"insignificant\"\n",
        "    else: result = result[0]\n",
        "\n",
        "    pairing_dict[pair][\"cons_pair\"] = result\n",
        "\n",
        "    result = sub_frame[\"pros_pair\"].dropna().to_list()\n",
        "    if len(result) == 0: result = \"insignificant\"\n",
        "    else: result = result[0]\n",
        "\n",
        "    pairing_dict[pair][\"pros_pair\"] = result\n",
        "\n",
        "  return pairing_dict"
      ],
      "metadata": {
        "id": "URF4LSvcphAQ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare data for export"
      ],
      "metadata": {
        "id": "Nlm_ljAMqG4G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_df_with_conflict_overview(p_value_adjust_df):\n",
        "  df_cleaned = p_value_adjust_df.copy().reset_index().rename(columns={\"index\":\"pair\"})\n",
        "  df_cleaned = df_cleaned[df_cleaned.p_value_adjusted < p_value]\n",
        "\n",
        "  # remove rows with pairs of equal labels\n",
        "  df_cleaned = df_cleaned[df_cleaned.pair.apply(lambda x: x.split(\" | \")[0] != x.split(\" | \")[-1])]\n",
        "  df_cleaned[\"conflict\"] = df_cleaned[\"pair\"].apply(lambda x: \"_\".join(sorted(x.split(\" | \"))))\n",
        "  df_cleaned = df_cleaned.sort_values(\"conflict\")\n",
        "  #high_df_cleaned = high_df_cleaned.set_index([\"tradeoff\", \"pair\"])\n",
        "  return df_cleaned\n",
        "\n",
        "\n",
        "\n",
        "def get_tradeoff_and_non_tradeoff_pairs(df_cleaned):\n",
        "  tradeoffs = [pair for pair in high_pairs if len(df_cleaned[df_cleaned[\"conflict\"] == pair]) == 2]\n",
        "  non_tradeoff_pairs = [pair for pair in high_pairs if len(df_cleaned[df_cleaned[\"conflict\"] == pair]) != 2]\n",
        "  return tradeoffs, non_tradeoff_pairs\n",
        "\n",
        "\n",
        "\n",
        "def create_final_tradeoff_df(df_cleaned, list_of_items):\n",
        "\n",
        "  tradeoff_df = df_cleaned[df_cleaned[\"conflict\"].isin(list_of_items)] #.set_index([\"tradeoff\", \"pair\"])\n",
        "  tradeoff_df[\"con\"] = tradeoff_df[\"pair\"].apply(lambda x: x.split(\" | \")[0])\n",
        "  tradeoff_df[\"pro\"] = tradeoff_df[\"pair\"].apply(lambda x: x.split(\" | \")[-1])\n",
        "  tradeoff_df[\"tradeoff\"] = tradeoff_df[\"conflict\"].apply(lambda x: x.replace(\"_\", \" vs. \"))\n",
        "  tradeoff_df = tradeoff_df.set_index([\"tradeoff\", \"con\", \"pro\"]).drop(\"pair\", axis=1).rename(columns = {\"COUNT\":\"count\", \"p_value\":\"p-value\", \"p_value_adjusted\":\"p-value adjusted\"})\n",
        "  return tradeoff_df\n",
        "\n",
        "\n",
        "\n",
        "def create_final_non_tradeoff_df(df_cleaned, list_of_items):\n",
        "\n",
        "  non_tradeoff_df = df_cleaned[df_cleaned[\"conflict\"].isin(list_of_items)] #\n",
        "  non_tradeoff_df[\"con\"] = non_tradeoff_df[\"pair\"].apply(lambda x: x.split(\" | \")[0])\n",
        "  non_tradeoff_df[\"pro\"] = non_tradeoff_df[\"pair\"].apply(lambda x: x.split(\" | \")[-1])\n",
        "  non_tradeoff_df = non_tradeoff_df.sort_values([\"con\", \"pro\"]).reset_index(drop=True).drop(\"pair\", axis=1) #.drop(\"conflict\", axis=1)\n",
        "  \n",
        "  non_tradeoff_df = non_tradeoff_df[[\"con\", \"pro\", \"COUNT\", \"p_value\", \"conflict\"]].rename(columns = {\"COUNT\":\"count\", \"p_value\":\"p-value\", \"p_value_adjusted\":\"p-value adjusted\"}).set_index([\"con\", \"pro\"])\n",
        "  return non_tradeoff_df\n",
        "\n",
        "def retrieve_good_examples(conflicts_df, n_of_examples):\n",
        "\n",
        "  frames = []\n",
        "\n",
        "  for tradeoff in conflicts_df.contradiction.unique():\n",
        "    frame = conflicts_df[conflicts_df.contradiction == tradeoff]\n",
        "    for contradiction_side in frame.contradiction_side.unique():\n",
        "      sub_frame = conflicts_df[conflicts_df.contradiction_side == contradiction_side]\n",
        "      frames.append(sub_frame.sort_values(\"combined_score\", ascending=False).iloc[:n_of_examples])\n",
        "\n",
        "  good_examples_df = pd.concat(frames)\n",
        "  good_examples_df[\"tradeoff\"] = good_examples_df.contradiction.apply(lambda x: x.replace(\"_\", \" vs. \"))\n",
        "  good_examples_df = good_examples_df.set_index([\"tradeoff\", \"cons_label\", \"pros_label\", \"contradiction\"])\n",
        "\n",
        "  return good_examples_df\n",
        "\n",
        "def add_in_column_frequency_class_to_tradeoff_dfs(df, pairing_dict):\n",
        "\n",
        "  df[\"cons pair\"] = df[\"conflict\"].apply(lambda x: pairing_dict.get(x)['cons_pair'] if x in pairing_dict.keys() else \"insignificant\")\n",
        "  df[\"pros pair\"] = df[\"conflict\"].apply(lambda x: pairing_dict.get(x)['pros_pair'] if x in pairing_dict.keys() else \"insignificant\")\n",
        "  return df\n",
        "\n"
      ],
      "metadata": {
        "id": "BrEKjRpjqGEz"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7vWCmShxW1O"
      },
      "source": [
        "## Load data\n",
        "\n",
        "Note that you will need access to Glassdoor's proprietary data to run the code. If you just want to test with a dummy frame, you will need the following columns:\n",
        "\n",
        "| reviewID | pros | cons | employerName |\n",
        "| --- | --- | --- | --- |\n",
        "| some number | some string | another string | another string |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "arzJSJ1qxO3H"
      },
      "outputs": [],
      "source": [
        "sector_name = \"Finance\"\n",
        "#sector_name = \"Manufacturing\"\n",
        "df = pd.read_csv(f\"/content/{sector_name}.csv\")\n",
        "print(len(df))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Execution"
      ],
      "metadata": {
        "id": "Vn3wEff_NLTt"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmukkDFCJad9"
      },
      "source": [
        "### Identify the labels (i.e., the categories that will serve as topics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fawgsw3oMmVQ"
      },
      "outputs": [],
      "source": [
        "random_seed = 37\n",
        "number_of_sampled_comments = 4000\n",
        "number_of_most_frequent_words = 15\n",
        "dissimilarity_threshold = 0.3\n",
        "stopwords_to_be_added = [\"lot\",\"con\",\"pro\", \"thing\"]\n",
        "\n",
        "labels_to_be_explicitly_ignored = [\"place\", '-pron-', 'great']\n",
        "labels, noun_df = get_most_representative_labels(df, number_of_sampled_comments, number_of_most_frequent_words, labels_to_be_explicitly_ignored, dissimilarity_threshold, stopwords_to_be_added, random_seed, use_similarity_reduction=True)\n",
        "labels_to_be_filtered = [\"nothing negative to say\", \"nothing positive to say\", \"none\"]\n",
        "labels = labels + labels_to_be_filtered\n",
        "labels = [x for x in labels if x not in labels_to_be_explicitly_ignored]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oF62zRYm0VBD"
      },
      "source": [
        "### Get the sentence embeddings for the pros and cons sections"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df = pd.read_csv(f\"/content/{sector_name}.csv\")\n",
        "\n",
        "reviewIDs = df[\"reviewID\"].to_list()\n",
        "pros = df.pros.to_list()\n",
        "cons = df.cons.to_list()\n",
        "\n",
        "pros_prefix = \"\"\n",
        "cons_prefix = \"\"\n",
        "\n",
        "embeddings_pros = model.encode([f'{pros_prefix} {x}'.strip() for x in pros],convert_to_tensor=True, device=0)\n",
        "embeddings_cons = model.encode([f'{cons_prefix} {x}'.strip() for x in cons],convert_to_tensor=True, device=0)\n"
      ],
      "metadata": {
        "id": "mQqQ_09dAsg5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get the sentence embeddings for the labels"
      ],
      "metadata": {
        "id": "n795z6AdNHbG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pro_labels = [f'good {x}' for x in labels]\n",
        "con_labels = [f'bad {x}' for x in labels]\n",
        "embeddings_labels_pro = model.encode(pro_labels,convert_to_tensor=True, device=0)\n",
        "embeddings_labels_con = model.encode(con_labels,convert_to_tensor=True, device=0)\n",
        "embeddings_labels = model.encode(labels,convert_to_tensor=True, device=0)"
      ],
      "metadata": {
        "id": "iZ_4ROZDFSQK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Identify pro-con pairs"
      ],
      "metadata": {
        "id": "qUFOXBT1o5cr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sim_thresh = 0.35\n",
        "minimal_n_of_pairings = 0\n",
        "number_of_iterations = 10000\n",
        "p_value = 0.05\n",
        "\n",
        "embeddings_labels_dict = {\n",
        "    \"embeddings_labels_pro\":embeddings_labels,\n",
        "    \"embeddings_labels_con\":embeddings_labels,\n",
        "}\n",
        "\n",
        "pros_df, cons_df, pros_cons_df, pros_cons_with_p_values_high_df, pros_cons_with_p_values_low_df, pair_high_adjust_df, pair_low_adjust_df, p_value_high_adjust_df, p_value_low_adjust_df, high_adjust_df, low_adjust_df, pair_high_adjust_df, pair_low_adjust_df = obtain_sector_contradictions(embeddings_pros, embeddings_cons, embeddings_labels_dict, labels, labels_to_be_filtered, reviewIDs, sim_thresh, p_value, random_seed, number_of_iterations)"
      ],
      "metadata": {
        "id": "KKN9ZX7sM31E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pair_high_adjust_df[\"sector\"] = sector_name\n",
        "pair_high_adjust_df[\"sim_thresh\"] = sim_thresh\n",
        "\n",
        "pair_low_adjust_df[\"sector\"] = sector_name\n",
        "pair_low_adjust_df[\"sim_thresh\"] = sim_thresh\n",
        "\n",
        "sector_contradictions = pair_high_adjust_df.pair_values_1.to_list()"
      ],
      "metadata": {
        "id": "5mHXF-F9nzwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "RsoodieCM7HX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "contras_examples_df = retrieve_examples_for_conflicts_in_contradictions(pros_cons_df, df, sector_contradictions)\n",
        "infrequent_examples_df = retrieve_examples_for_conflicts(pros_cons_df, df, low_adjust_df[\"index\"].to_list())\n",
        "frequent_examples_df = retrieve_examples_for_conflicts(pros_cons_df, df, high_adjust_df[\"index\"].to_list())"
      ],
      "metadata": {
        "id": "M5cbDc5f0t97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "companies_contradiction_side_count_df_1 = contras_examples_df.groupby([\"employerName\", \"contradiction\", \"contradiction_side\"]).count()[[\"reviewID\"]].rename(columns={\"reviewID\":\"contra_side_count\"})\n",
        "companies_contradiction_side_count_df_2 = contras_examples_df.groupby([\"employerName\", \"contradiction\"]).count()[[\"reviewID\"]].rename(columns={\"reviewID\":\"contra_count\"})\n",
        "companies_contradiction_df = pd.merge(companies_contradiction_side_count_df_1, companies_contradiction_side_count_df_2, left_index=True, right_index=True)\n",
        "companies_contradiction_df = companies_contradiction_df.reset_index()\n",
        "companies_contradiction_df = add_missing_contradiction_sides(companies_contradiction_df)\n",
        "companies_contradiction_df[\"ratio\"] = companies_contradiction_df[\"contra_side_count\"] / companies_contradiction_df[\"contra_count\"]\n",
        "selected_sides = retain_only_one_side_of_contradictions(companies_contradiction_df)\n",
        "companies_contradiction_df = companies_contradiction_df[companies_contradiction_df.contradiction_side.isin(selected_sides)]\n"
      ],
      "metadata": {
        "id": "YPpWHKg3KFK2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "P3rScGv3nfw2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_of_bins = 6\n",
        "bin_dict = {1/n_of_bins + x/n_of_bins : i+1 for i, x in enumerate(range(n_of_bins))}\n",
        "\n",
        "companies_contradiction_df[\"ratio_bin\"] = companies_contradiction_df.ratio.apply(lambda x: ratio_into_bin(x, bin_dict))\n",
        "frame_1 = filter_by_minimal_occurence(companies_contradiction_df, 1)\n",
        "frame_2 = filter_by_minimal_occurence(companies_contradiction_df, 2)\n",
        "\n",
        "frame_1_bin_counts_df = frame_1.groupby([\"contradiction\", \"contradiction_side\", \"ratio_bin\"]).count()[[\"employerName\"]].rename(columns={\"employerName\":\"bin_count\"}).reset_index()\n",
        "frame_2_bin_counts_df = frame_2.groupby([\"contradiction\", \"contradiction_side\", \"ratio_bin\"]).count()[[\"employerName\"]].rename(columns={\"employerName\":\"bin_count\"}).reset_index()"
      ],
      "metadata": {
        "id": "Pz2VC8sfD4pM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "number_of_columns = 2\n",
        "\n",
        "contradiction_sides = list(companies_contradiction_df.contradiction_side.unique())\n",
        "number_of_sector_contradictions = len(contradiction_sides)\n",
        "number_of_rows = int(np.ceil(number_of_sector_contradictions / number_of_columns))\n",
        "\n",
        "fig = make_subplots(rows=number_of_rows, cols=number_of_columns, vertical_spacing = 0.15, horizontal_spacing = 0.15,\n",
        "                    subplot_titles=[x.replace(\"|\", \"vs.\") for x in contradiction_sides])\n",
        "\n",
        "contra_counter = 0\n",
        "\n",
        "for i, number_of_row in enumerate(range(1,number_of_rows+1)):\n",
        "  for j, number_of_column in enumerate(range(1,number_of_columns+1)):\n",
        "\n",
        "    if contra_counter < number_of_sector_contradictions:\n",
        "      contradiction_side = contradiction_sides[contra_counter]\n",
        "      contradiction_side_label = f'bad: {contradiction_side.split(\" | \")[0]}<br>good: {contradiction_side.split(\" | \")[-1]}'\n",
        "      contradiction_side_opposite = f'bad: {contradiction_side.split(\" | \")[-1]}<br>good: {contradiction_side.split(\" | \")[0]}'\n",
        "      \n",
        "      frame = frame_1_bin_counts_df[frame_1_bin_counts_df.contradiction_side == contradiction_side]\n",
        "      frame = add_missing_bins(frame)\n",
        "\n",
        "      if (i == 0) & (j == 0): showlegend = True\n",
        "      else: showlegend = False\n",
        "\n",
        "      fig.add_trace(\n",
        "          go.Bar(\n",
        "              x=frame[\"ratio_bin\"], \n",
        "              y=frame[\"bin_count\"], \n",
        "              marker=dict(color=\"blue\", opacity=0.5),\n",
        "              showlegend=showlegend,\n",
        "              name=\"at least 1 instance\"\n",
        "              ),\n",
        "              row=number_of_row, col=number_of_column,\n",
        "              \n",
        "      )\n",
        "\n",
        "      frame = frame_2_bin_counts_df[frame_2_bin_counts_df.contradiction_side == contradiction_side]\n",
        "      frame = add_missing_bins(frame)\n",
        "\n",
        "      fig.add_trace(\n",
        "          go.Bar(\n",
        "              x=frame[\"ratio_bin\"], \n",
        "              y=frame[\"bin_count\"], \n",
        "              marker=dict(color=\"red\", opacity=0.5),\n",
        "              showlegend=showlegend,\n",
        "              name=\"at least 2 instances\"\n",
        "              ),\n",
        "              row=number_of_row, col=number_of_column,\n",
        "              \n",
        "      )\n",
        "\n",
        "      fig.update_yaxes(\n",
        "          title_text=\"# of companies\", \n",
        "          gridcolor=\"grey\",\n",
        "          type=\"log\",\n",
        "          row=number_of_row, \n",
        "          col=number_of_column)\n",
        "      \n",
        "      fig.update_xaxes(\n",
        "          #title_text=\"yaxis 1 title\", \n",
        "          ticktext=[f\"<br>100 - 83%<br><br>{contradiction_side_opposite}\",\n",
        "                    f\"<br>50%<br><br>both combinations\",\n",
        "                    f\"<br>83 - 100%<br><br>{contradiction_side_label}\"],\n",
        "          tickvals=[1, 3.5, 6],\n",
        "          range=[0, 7],\n",
        "          row=number_of_row, \n",
        "          col=number_of_column)\n",
        "\n",
        "    contra_counter = contra_counter + 1\n",
        "\n",
        "\n",
        "fig.update_layout(\n",
        "    height=number_of_rows*400, \n",
        "    width=number_of_columns*550,\n",
        "    barmode=\"overlay\",\n",
        "    #title=dict(text=f\"Tradeoffs in {sector_name} from the perspective of employees\", x=0.5, xanchor=\"center\", yanchor=\"top\", y=0.99),\n",
        "    margin=dict(\n",
        "        t=100, \n",
        "        b=150\n",
        "        ),\n",
        "    legend=dict(orientation=\"h\", xanchor = \"left\", x = 0, y= -0.7),\n",
        "    plot_bgcolor = \"rgba(0,0,0,0)\")\n",
        "\n",
        "\n",
        "fig.show()\n",
        "fig.write_image(f'{sector_name}_figure.png', engine=\"kaleido\", scale=2)\n",
        "fig.write_image(f'{sector_name}_figure.svg', engine=\"kaleido\")\n"
      ],
      "metadata": {
        "id": "FIwt6XuZGZ-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pros_pairs_df, pros_frame, pros_pair_count_dict = identify_pairs_in_either_pros_or_cons_column(pros_df, \"pros\", labels_to_be_filtered)\n",
        "cons_pairs_df, cons_frame, cons_pair_count_dict = identify_pairs_in_either_pros_or_cons_column(cons_df, \"cons\", labels_to_be_filtered)"
      ],
      "metadata": {
        "id": "OJMHgfe4Get-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pros_frame[\"random_label\"] = np.random.permutation(pros_frame[\"pros_label\"])\n",
        "pros_frame[\"random_pair\"] = sorted(pros_frame[\"random_label\"] + \"_\" + pros_frame[\"pros_label\"])"
      ],
      "metadata": {
        "id": "km9bFHToGewm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "z_score = 1.96\n",
        "summary_of_significant_pairings_in_same_column_df = create_summary_of_summary_of_significant_pairings_in_same_column(pros_frame, pros_pairs_df, cons_frame, cons_pairs_df, pros_pair_count_dict, cons_pair_count_dict, number_of_iterations)\n",
        "pairing_dict = create_dict_for_pairing_frequency_classes(summary_of_significant_pairings_in_same_column_df)"
      ],
      "metadata": {
        "id": "k08pDzirZkb7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grouped_comp_contra_df = frame_2.set_index([\"contradiction\", \"employerName\"])\n",
        "contradictions = grouped_comp_contra_df.index.get_level_values(0).unique()\n",
        "\n",
        "frames = []\n",
        "\n",
        "for contradiction in contradictions:\n",
        "\n",
        "  frames.append(grouped_comp_contra_df.loc[contradiction][[\"ratio\"]].rename(columns={\"ratio\":contradiction}))\n",
        "\n",
        "merged_df = frames[0]\n",
        "for frame in frames[1:]:\n",
        "  merged_df = pd.merge(merged_df, frame, left_index=True, right_index=True, how=\"outer\")\n",
        "corr_df = merged_df.corr()"
      ],
      "metadata": {
        "id": "iRTeE0mosLDM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mask = np.triu(np.ones_like(corr_df, dtype=bool))\n",
        "rLT = corr_df.applymap(lambda x: np.abs(x)).mask(mask)\n",
        "\n",
        "fig = go.Figure(data=go.Heatmap(\n",
        "                   z=rLT,\n",
        "                   x=[x.replace(\"_\", \" vs. \") for x in rLT.columns],\n",
        "                   y=[x.replace(\"_\", \" vs. \") for x in rLT.columns],\n",
        "                   hoverongaps = False,\n",
        "                   #colorscale = 'RdBu',\n",
        "                   ))\n",
        "\n",
        "fig.update_xaxes(tickangle=90, showgrid=False)\n",
        "fig.update_yaxes(showgrid=False)\n",
        "\n",
        "\n",
        "margin = 250\n",
        "fig.update_layout(\n",
        "    width=600,\n",
        "    height=600,\n",
        "    plot_bgcolor=\"rgba(0,0,0,0)\",\n",
        "    margin=dict(pad=20, b=margin, l=200),\n",
        "    yaxis_autorange='reversed',\n",
        "    \n",
        "    \n",
        ")\n",
        "fig.show()\n",
        "\n",
        "fig.write_image(f'{sector_name}_figure_corr_matrix.png', engine=\"kaleido\", scale=3)"
      ],
      "metadata": {
        "id": "HP_4zOrUtd6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels_used = [x for x in labels if x not in labels_to_be_filtered + labels_to_be_explicitly_ignored]\n",
        "labels_used"
      ],
      "metadata": {
        "id": "fP5_WzqPJRVY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "high_df_cleaned = clean_df_with_conflict_overview(p_value_high_adjust_df)\n",
        "low_df_cleaned = clean_df_with_conflict_overview(p_value_low_adjust_df)\n",
        "\n",
        "high_pairs = high_df_cleaned[\"conflict\"].unique()\n",
        "\n"
      ],
      "metadata": {
        "id": "ATH2zZKBqrtE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tradeoff_high_df = create_final_tradeoff_df(high_df_cleaned, tradeoffs_high)\n",
        "tradeoff_low_df = create_final_tradeoff_df(low_df_cleaned, tradeoffs_low)\n",
        "\n",
        "non_tradeoff_high_df = create_final_non_tradeoff_df(high_df_cleaned, non_tradeoff_pairs_high)\n",
        "non_tradeoff_low_df = create_final_non_tradeoff_df(low_df_cleaned, low_df_cleaned.conflict.to_list())\n",
        "\n",
        "tradeoffs_high, non_tradeoff_pairs_high = get_tradeoff_and_non_tradeoff_pairs(high_df_cleaned)\n",
        "tradeoffs_low, non_tradeoff_pairs_low = get_tradeoff_and_non_tradeoff_pairs(low_df_cleaned)"
      ],
      "metadata": {
        "id": "FLNCEENhdx7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "contras_good_examples_df = retrieve_good_examples(contras_examples_df, 3)\n",
        "infrequent_good_examples_df = retrieve_good_examples(infrequent_examples_df, 3)\n",
        "frequent_good_examples_df = retrieve_good_examples(frequent_examples_df, 3)"
      ],
      "metadata": {
        "id": "uaGlOWCC86Kx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "non_tradeoff_high_df = add_in_column_frequency_class_to_tradeoff_dfs(non_tradeoff_high_df, pairing_dict)\n",
        "tradeoff_high_df = add_in_column_frequency_class_to_tradeoff_dfs(tradeoff_high_df, pairing_dict)\n",
        "non_tradeoff_low_df = add_in_column_frequency_class_to_tradeoff_dfs(non_tradeoff_low_df, pairing_dict)\n",
        "tradeoff_low_df = add_in_column_frequency_class_to_tradeoff_dfs(tradeoff_low_df, pairing_dict)"
      ],
      "metadata": {
        "id": "DeDTbpIA9Xp2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "--QyI7lTGHTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Output of summary file"
      ],
      "metadata": {
        "id": "RI-hn3Y_rtQr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content\"\n",
        "\n",
        "with pd.ExcelWriter(f\"{path}/{sector_name}_{str(sim_thresh).replace('.', 'dot')}_summary.xlsx\", engine='xlsxwriter') as writer:\n",
        "  tradeoff_high_df.to_excel(writer, \"tradeoffs\")\n",
        "  non_tradeoff_high_df.to_excel(writer, \"non-tradeoff pairs\")\n",
        "\n",
        "  if len(tradeoff_low_df) > 0: tradeoff_low_df.to_excel(writer, \"infreq tradeoffs\")\n",
        "  non_tradeoff_low_df.to_excel(writer, \"infreq non-tradeoff pairs\")\n",
        "  \n",
        "  contras_good_examples_df.to_excel(writer, \"examples contras\")\n",
        "  frequent_good_examples_df.to_excel(writer, \"examples frequent\")\n",
        "  infrequent_good_examples_df.to_excel(writer, \"examples infrequent\")\n",
        "  \n",
        "  contras_examples_df.to_excel(writer, \"all tradeoff comms\")\n",
        "  infrequent_examples_df.to_excel(writer, \"all infrequent comms\")\n",
        "  frequent_examples_df.to_excel(writer, \"all frequent comms\")\n",
        "  pd.DataFrame().to_excel(writer, \"figures\")\n",
        "\n",
        "  summary_of_significant_pairings_in_same_column_df.to_excel(writer, \"pair frequencies\")\n",
        "  pd.DataFrame({\"labels\":labels_used}).to_excel(writer, \"list of labels\")\n",
        "\n",
        "  workbook  = writer.book\n",
        "  worksheet = writer.sheets['figures']\n",
        "\n",
        "  worksheet.insert_image('D3', f'{sector_name}_figure.png')\n",
        "  worksheet.insert_image('D20', f'{sector_name}_figure_corr_matrix.png')"
      ],
      "metadata": {
        "id": "KWiWRsQrku6A"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Tradeoff-detection.ipynb",
      "provenance": [],
      "mount_file_id": "1xu5pG0EvpTosCjTXn4ohKPcG2DxVMB2J",
      "authorship_tag": "ABX9TyNm5+wfjIKXAYpMWNbsKkhy",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}